
1) Tell me something about you. (need to have a good flow for points)
2) Tell me your Day-to-day activities (confidently say 12 activities)
Suggesstions : In real time scenario , 12 to 15 daytoday activities.
3) Do you know what are under-replicated blocks
                get the full details of the files which are causing your problem using
                $hdfs fsck / -files -blocks -locations
               
                To Fix under-replicated blocks in HDFS, below is quick instruction to use:
               
                su - <$hdfs_user>
               #the below command will list all under replicated files to '/tmp/under_replicated_files'
                $ hdfs fsck / | grep 'Under replicated' | awk -F':' '{print $1}' >> /tmp/under_replicated_files
 
                $ for hdfsfile in `cat /tmp/under_replicated_files`; do echo "Fixing $hdfsfile :" ;  hadoop fs -setrep 3 $hdfsfile;done
                => Namenode WebUI , blocks are hyperlink to get more information.      
                3a) How will you monitor the under-replicated blocks,as to which blocks are under replicated (using CLI and CM)
                                =>CLI
                                	$ hdfs fsck -list-corruptfileblocks
                                             The filesystem under path '/' has 2114 CORRUPT files
                                If files are not important can be deleted by using the command
                                                $ hdfs fsck / -delete
4) if there is a job submitted and it is working slow/hung.How will you monitor the job
                ->Resource Manager UI/YARN Application UI
                                Check the number of mappers and reducers available, or job queues.
5) Have you done Commissioning/De-Commissioning
                5a) What is Pre-requisites of Commissioning/De-Commissioning
		Commission and Decommission a host using Cloudera Manager.
			-Adding a new Node/Commission a host using Cloudera Manager.
        	        Prerequisites for adding a new host in Cluster.
        	        -SELinux,vm.swappiness,Transparent Huge pages,IpTables off,NTP configured.
        	        Go to hosts > Add New Hosts to Cluster.
        	        Provide FQDN and search the host.
        	        Install the JDK and Cloudera Manager Agent, this will be same as version on other nodes.
        	        After above step, the CDH Parcels will be distributed and Activated on the new nodes.
        	        After above step, host inspector gives brief information on various services e.g swappiness.
        	        Cloudera manager provides the commands to be executed for prerequisites on the newly added node.
        	        Now the new nodes is added in the cluster, but there are no roles assigned to the new node. 
		-Decommission a host using Cloudera Manager.
                	Stopping and decomminssioning all roles running on that particular node.
                	Applies to HDFS,datanode,Nodemanager,YARN,HBase Region Service.
                	if host has any service other than above , then we need to manually decommission the service/roles.
                	Once all services/roles are stopped, then host can be decommissioned.
                	Go to Hosts>Instances>select any instance >Actions > Hosts Decommission.
               	
                	The status can be checked on Namenode WebUI.
 	
                	Further if the node needs to be removed from the cluster.
                	Go to Hosts>Instances>select any instance >Actions > Remove host from cluster.
 	
                	Further if the node needs to be removed from the Cloudera Manager
                	Go to Hosts>Instances>select any instance >Actions > Remove host from Cloudera Manager.

                =>Cluster should be in maintenance mode.
                =>Using the Cloudera manager explain steps.
6) Tell me about your Capacity Planning
                                Daily Data : 10GB , with repliation 30GB
                                Monthly Data with replication : 900GB
                                Yearly Data : 10.5 TB DFS
                                30% Non-DFS to be added to above value : 10.5+3.2 = 13.7TB
                                10% buffer space i.e 1TB , so total data for 1 year is 16TB
                               
                                Total Number of Nodes in Cluster : 13
                                Master Nodes : 4 (NN,Standby NN,RM , Standby RM)
                                8 DN - 2TB storage each
                                1 Edge Node : Cloudera Manager
                               
                6a) Why do you require Non-DFS space in cluster
                                =>Non DFS used is any data in the filesystem of the data node(s) that isn't in dfs.data.dirs.
                                This would include log files, mapreduce shuffle output and local copies of data files (if you put them on a data node).
                                Use du or a similar tool to see whats taking up the space in your filesystem.
7) Tell me about your cluster configuration.
8) What is HA?
                a)How to configure HA in Cloudera Manager.
                                =>Go to the HDFS service>Select Actions > Enable High Availability.
                                Give any appropriate NameService Name.
                                -Now Assign Role i.e Select Namenode Host for Standby Host
                                -Select Journal Nodes (recommended 3 or more)should be on NN,Standby NN and RM node, click on continue.
                                Specify the below property :
                                ->dfs.namenode.name.dir : can be same as previous for both nodes (node01,node02).
                                ->dfs.journalnodes.edits.dir : specify directory name (/dfs/edits) for all 3 nodes.
                                Click on Continue
                                Below are summary steps that Cloudera Manager does to Enable High Availability:
                                -Check if namenode directory is empty/present/clear on new Standby NameNode
                                -Check if dfs.journalnodes.edits.dir directory is empty/present/clear on all journal nodes.
                               
                                Extra Options
                                A screen showing the hosts that are eligible to run a standby NameNode and the JournalNodes displays.
                b)Why there is need for 3 Zookeeper in HA?
			=>Apache ZooKeeper is a distributed coordination service. It is a framework that can be used to build distributed applications by providing a set of 				services such as a name service, locking,synchronization, configuration management, and leader election services. These services are explained as follows:
			=>Zookeeper requires that you have a quorum of servers up, where quorum is ceil(N/2). For a 3 server ensemble, that means 2 servers must be up at any time, 				for a 5 server ensemble, 3 servers need to be up at any time.
9) Tell me the YARN job Flow.
11) Application Master is inside container OR container is inside Application Master?                      
                =>When Job is submitted, RM contacts NM, to open a container, Application Master executes inside the container.
12) What happens to the application master/container when a job is submitted/completed.
13) Do you know about Trash. Have you configured it (CLI/CM)? What are the steps to configure in CM and using CLI?
                => Configuring HDFS Trash
                                The Hadoop trash feature helps prevent accidental deletion of files and directories. When you delete a file in HDFS,
                                the file is not immediately expelled from HDFS. Deleted files are first moved to the /user/<username>/.Trash/Current directory,
                                with their original filesystem path being preserved. After a user-configurable period of time (fs.trash.interval),
                                a process known as trash checkpointing renames the Current directory to the current timestamp, that is, /user/<username>/.Trash/<timestamp>.
                                The checkpointing process also checks the rest of the .Trash directory for any existing timestamp directories and removes them from HDFS permanently.
                                You can restore files and directories in the trash simply by moving them to a location outside the .Trash directory.
                               
                                Important:
                                The trash feature is disabled by default. Cloudera recommends that you enable it on all production clusters.
                                The trash feature works by default only for files and directories deleted using the Hadoop shell.
                                Files or directories deleted programmatically using other interfaces (WebHDFS or the Java APIs, for example) are not moved to trash,
                                even if trash is enabled, unless the program has implemented a call to the trash functionality.
                                (Hue, for example, implements trash as of CDH 4.4.).Users can bypass trash when deleting files using the shell by specifying
                                the -skipTrash option to the hadoop fs -rm -r command. This can be useful when it is necessary to delete files that are too large
                                for the user's quota.
                               
                => Configuring HDFS Trash Using Cloudera Manager
                                Enabling and Disabling Trash
                                                Go to the HDFS service > Configuration tab >Select Scope > Gateway >Select or clear the Use Trash checkbox.
                                                If more than one role group applies to this configuration, edit the value for the appropriate role group.
                                                See Modifying Configuration Properties Using Cloudera Manager.
                                                Click Save Changes to commit the changes.Restart the cluster and deploy the cluster client configuration
                                Setting the Trash Interval
                                                Go to the HDFS service >Configuration tab > Select Scope > NameNode.
                                                Specify the Filesystem Trash Interval property, which controls the number of minutes after which a trash checkpoint
                                                directory is deleted and the number of minutes between trash checkpoints.
                                                For example, to enable trash so that deleted files are deleted after 24 hours,set the value of the Filesystem Trash Interval property to 1440.
                                                Note: The trash interval is measured from the point at which the files are moved to trash,
                                                not from the last time the files were modified.If more than one role group applies to this configuration,
                                                edit the value for the appropriate role group. See Modifying Configuration Properties Using Cloudera Manager.
                                                Click Save Changes to commit the changes.Restart all NameNodes.
                                               
                                                fs.trash.interval -> Number of minutes after which the checkpoint gets deleted. If zero, the trash feature is disabled.
                                                fs.trash.checkpoint.interval ->    Number of minutes between trash checkpoints. Should be smaller or equal to fs.trash.interval.
                                                                                                                                                                                Every time the checkpointer runs it creates a new checkpoint out of current and removes checkpoints
                                                                                                                                                                                created more than fs.trash.interval minutes ago.
14) What will happen if your Cloudera Manager goes down?
                => Login CM-node and troubleshoot in CLI to check if Cloudera Manager Server is running.
                                Check if cloudera-scm-server is running on port (7180,7182)
15) What is the difference between JobID and ApplicationID?
                =>           If you use YARN version of kill command, the resource manager kills the AM without AM's knowledge so it will not generate history.
                                If you use hadoop job -kill job_id the killing process goes through the AM and it will allow to generate history.
                                In terms of YARN, the programs that are being run on a cluster are called applications. In terms of MapReduce they are called jobs.
                                So, if you are running MapReduce on YARN, job and application are the same thing (if you take a close look, job ids and application ids are the same).                  
                                MapReduce job consists of several tasks (they could be either map or reduce tasks). If a task fails, it is launched again on another node.
                                Those are task attempts.
 
Container is a YARN term. This is a unit of resource allocation. For example, MapReduce task would be run in a single container.
 
16) Tell me in brief about your project.
                Our project focuses on log aggregation of users Swipe-In/Swipe Out Data for a day, for real time analysis of an employee in the company.
                Sensor logs are generated and are dumped into hdfs using flume for analysis, the analysed data is used to help employee track time and the same
                is used for client billing. We have RDBMS data ingestion on month ends , to get billing information to client and the data from logs is mapped with
                the billing RDBMS.
17) Tell me the services configured in your cluster(HDFS,YARN,FLUME,HIVE,HUE,SQOOP,OOZIE).
18) Are you having structure data? Which service is used for it.
                We are having client billing and timesheet related RDBMS data loaded into hadoop using SQOOP, we use sqoop import command to get the data into hive.
                sqoop import --connect jdbc:mysql://localhost:3306/sqoop --username root -P --split-by id --columns id,name --table customer --target-dir /user/cloudera/ingest/raw/customers
                --fields-terminated-by ","
                --hive-import
                --create-hive-table
                --hive-table sqoop_workspace.customers.
                Here’s what each individual Sqoop command option means:       
                                connect – Provides jdbc string
                                username – Database username
                                -P  – Will ask for the password in console. Alternatively you can use –password but this is not a good practice as its visible in your job execution logs and asking for trouble. One way to deal with this is store database passwords in a file in HDFS and provide at runtime.
                                table – Tells the computer which table you want to import from MySQL. Here, it's customer.
                                split-by – Specifies your splitting column. I am specifying id here.
                                target-dir – HDFS destination directory.
                                fields-terminated-by – I have specified comma (as by default it will import data into HDFS with comma-separated values)
                                hive-import – Import table into Hive (Uses Hive’s default delimiters if none are set.)
                                create-hive-table – Determines if set job will fail if a Hive table already exists. It works in this case.
                                hive-table – Specifies <db_name>.<table_name>. Here it's sqoop_workspace.customers, where sqoop_workspace is my database and customers is the table name.
19) Tell me how sqoop works?
                                Sqoop uses MapReduce to import and export the data, which provides parallel operation as well as fault tolerance.
                                http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html#_controlling_parallelism
20) How do you configure Flume?
                => Flume Agent consists of Source , Channel , Sink.
21)In HA, what is the use of Quorum Journal Nodes?
22) Do you know about Namenode Fencing?
23) Do you know about Split Brain Condition ?
                => In HA, if active NN goes down, Standby NN becomes Active, and if both NN become active , the fencing method is the solution to avoid the split brain scenario.
24)Linux commands:
                24a) Command to check the port number of a service                    
                                => netstat | grep PORTNUMBER
                24b) Command to check Memory in linux
                                => vmstat , free
25) Have you worked on Hadoop 1. Give me difference between Hadoop v1 and v2?
26) what are the schedulars? (FIFO,Fair,Capacity)
27) Do you know what is ContainerZero?
                =>ContainerZero is first container launched when a job is submitted.
28) Have you generated any reports, (Daily Reports)
                =>Main Menu -> Clusters-> Reports
29)





----------------INFO-------------------------
 
 
-Using snapshot using CM to Back Up data
                -Enable a directory for snapshot.
                -Go to HDFS > File Browser > Particular Directory > Enable Snapshot
                -Snapshot Policy can be created to create a snapshot on regular intervals.
                -To create a Snapshot Policy, go to Main Menu > Backups > Snapshots > Create snapshot Policy.
                -Select particukar service to take snapshot of (HDFS, HBase , Hive)
                -Select daily,weekly,monthly as per requirement to enable the Snapshot Policy.
               
                -To restore files from snapshot, Go to HDFS > File Browser > Directory > Restore Directory from Snapshot>Select snapshot from dropdown list>Restore.
